{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to drive and import dataset"
      ],
      "metadata": {
        "id": "LgePOUnH7oHL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxV-33ADN1VZ",
        "outputId": "aef4416c-ed1b-4267-a36a-2e3d13ba2ef9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nz_zA4blN119",
        "outputId": "f8b7f1f8-1162-4440-bcec-0acf4be09a0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "\n",
        "%cp -r \"/content/gdrive/MyDrive/Afstudeerproject/Pictures/\" \"/content/\"\n",
        "%cp -r \"/content/gdrive/MyDrive/Afstudeerproject/output_annotations.json\" \"/content/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1Pnk1-tRON8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Path to your pictures directory\n",
        "pictures_dir = \"/content/Pictures/\"\n",
        "\n",
        "# Loop through all files in the directory\n",
        "for filename in os.listdir(pictures_dir):\n",
        "    if filename.startswith(\"Copy of \"):\n",
        "        # Construct new filename by removing \"Copy of \"\n",
        "        new_filename = filename.replace(\"Copy of \", \"\", 1)\n",
        "\n",
        "        # Full paths for old and new names\n",
        "        old_path = os.path.join(pictures_dir, filename)\n",
        "        new_path = os.path.join(pictures_dir, new_filename)\n",
        "\n",
        "        # Rename the file\n",
        "        os.rename(old_path, new_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define dataset"
      ],
      "metadata": {
        "id": "L3OEkEeT7stA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZKCSMlhNESa"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision.tv_tensors import BoundingBoxes\n",
        "from torch import nn\n",
        "\n",
        "# Load JSON annotation file\n",
        "with open(\"output_annotations.json\", \"r\") as f:\n",
        "    plant_data = json.load(f)\n",
        "\n",
        "# Map image IDs to file names\n",
        "image_id_to_filename = {img[\"id\"]: img[\"file_name\"] for img in plant_data[\"images\"]}\n",
        "\n",
        "# Get image sizes\n",
        "image_id_to_size = {img[\"id\"]: (img[\"width\"], img[\"height\"]) for img in plant_data[\"images\"]}\n",
        "\n",
        "# Organize annotations by image_id\n",
        "image_annotations = {img_id: [] for img_id in image_id_to_filename.keys()}\n",
        "for ann in plant_data[\"annotations\"]:\n",
        "    image_annotations[ann[\"image_id\"]].append(ann)\n",
        "\n",
        "class PlantDataset(Dataset):\n",
        "    def __init__(self, img_dir, annotations, img_id_to_filename, img_id_to_size, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.annotations = annotations\n",
        "        self.img_id_to_filename = img_id_to_filename\n",
        "        self.img_id_to_size = img_id_to_size\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_id_to_filename)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get image ID and file path\n",
        "        img_id = list(self.img_id_to_filename.keys())[idx]\n",
        "        img_path = os.path.join(self.img_dir, self.img_id_to_filename[img_id])\n",
        "\n",
        "        # Load image as PIL (don't resize here)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        orig_w, orig_h = self.img_id_to_size[img_id]\n",
        "\n",
        "        # Get original annotations (no scaling yet)\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for ann in self.annotations[img_id]:\n",
        "            x, y, w, h = ann[\"bbox\"]\n",
        "            boxes.append([x, y, x + w, y + h])  # Original coordinates\n",
        "            labels.append(ann[\"category_id\"])\n",
        "\n",
        "        # Convert to tensors\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        # Wrap boxes in BoundingBoxes for transform compatibility\n",
        "        target = {\n",
        "            \"boxes\": BoundingBoxes(boxes, format=\"XYXY\", canvas_size=(orig_h, orig_w)),\n",
        "            \"labels\": labels\n",
        "        }\n",
        "\n",
        "        # Apply transforms (will handle resizing and box scaling)\n",
        "        if self.transform:\n",
        "            image, target = self.transform(image, target)\n",
        "\n",
        "\n",
        "        # Convert boxes back to tensor\n",
        "        target[\"boxes\"] = target[\"boxes\"].data  # Convert BoundingBoxes -> tensor\n",
        "\n",
        "        return image, target"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define transforms"
      ],
      "metadata": {
        "id": "MJq0Dmmm72CC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms.v2 as transforms_v2\n",
        "\n",
        "# Define transforms\n",
        "train_transform = transforms_v2.Compose([\n",
        "    transforms_v2.ToTensor(),\n",
        "    transforms_v2.RandomHorizontalFlip(p=0.5),\n",
        "    transforms_v2.RandomVerticalFlip(p=0.3),\n",
        "    transforms_v2.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.02),\n",
        "    transforms_v2.RandomAffine(\n",
        "        degrees=15,\n",
        "        translate=(0.1, 0.1),\n",
        "        scale=(0.8, 1.2),  # More aggressive scaling\n",
        "        shear=5\n",
        "    ),\n",
        "    transforms_v2.Resize((640, 640), antialias=True),\n",
        "    transforms_v2.RandomErasing(p=0.3, scale=(0.02, 0.15)),\n",
        "    transforms_v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "val_transform = transforms_v2.Compose([\n",
        "    transforms_v2.ToTensor(),\n",
        "    transforms_v2.Resize((640, 640), antialias=True),\n",
        "    transforms_v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Original dataset (no transforms)\n",
        "full_dataset = PlantDataset(\n",
        "    img_dir=\"/content/Pictures/\",\n",
        "    annotations=image_annotations,\n",
        "    img_id_to_filename=image_id_to_filename,\n",
        "    img_id_to_size=image_id_to_size,\n",
        "    transform=None  # Transforms applied later\n",
        ")\n",
        "\n",
        "# Split datasets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Assign transforms\n",
        "train_dataset.dataset.transform = train_transform\n",
        "val_dataset.dataset.transform = val_transform"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VoOAU2c6ptX",
        "outputId": "3fbf7e05-ac53-4ede-9b24-9e68fa194252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7W2TsOxNWOz"
      },
      "source": [
        "## Define model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBaS_rzJNZ0D"
      },
      "outputs": [],
      "source": [
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "\n",
        "class FastRCNNPredictorWithDropout(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "        self.cls_score = nn.Sequential(\n",
        "            nn.Linear(in_channels, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),  # Higher dropout for small dataset\n",
        "            nn.Linear(1024, num_classes)\n",
        "        )\n",
        "        self.bbox_pred = nn.Linear(in_channels, num_classes * 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        scores = self.cls_score(x)\n",
        "        bbox_deltas = self.bbox_pred(x)\n",
        "        return scores, bbox_deltas\n",
        "\n",
        "num_classes = len(plant_data[\"categories\"]) + 1  # Background + classes\n",
        "train_dataloader = DataLoader(train_dataset,\n",
        "                        batch_size=8,\n",
        "                        num_workers=2,\n",
        "                        shuffle=True,\n",
        "                        collate_fn=lambda x: tuple(zip(*x)),\n",
        "                        pin_memory=True)\n",
        "\n",
        "model = fasterrcnn_resnet50_fpn(weights='DEFAULT')\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictorWithDropout(in_features, num_classes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX4d4WgkNfpZ"
      },
      "source": [
        "Train model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_--ephXNA9D8",
        "outputId": "24715b19-a5a7-4cbc-b12c-0538af6e441d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.6.0+cu124)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (0.14.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2XfiKAFNdm1",
        "outputId": "cfd6bfdd-246e-4163-c1ba-69e19a1ac2d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 20.8110\n",
            "Epoch 1 Metrics:\n",
            " - mAP@0.5: 0.2388\n",
            " - mAP@0.75: 0.0155\n",
            "Epoch 2, Loss: 20.6640\n",
            "Epoch 2 Metrics:\n",
            " - mAP@0.5: 0.2608\n",
            " - mAP@0.75: 0.0366\n",
            "Epoch 3, Loss: 20.5257\n",
            "Epoch 3 Metrics:\n",
            " - mAP@0.5: 0.3050\n",
            " - mAP@0.75: 0.0518\n",
            "Epoch 4, Loss: 19.9779\n",
            "Epoch 4 Metrics:\n",
            " - mAP@0.5: 0.3366\n",
            " - mAP@0.75: 0.0641\n",
            "Epoch 5, Loss: 19.6852\n",
            "Epoch 5 Metrics:\n",
            " - mAP@0.5: 0.3682\n",
            " - mAP@0.75: 0.0867\n",
            "Epoch 6, Loss: 19.1765\n",
            "Epoch 6 Metrics:\n",
            " - mAP@0.5: 0.4384\n",
            " - mAP@0.75: 0.1179\n",
            "Epoch 7, Loss: 18.5204\n",
            "Epoch 7 Metrics:\n",
            " - mAP@0.5: 0.4745\n",
            " - mAP@0.75: 0.1082\n",
            "Epoch 8, Loss: 17.6236\n",
            "Epoch 8 Metrics:\n",
            " - mAP@0.5: 0.5076\n",
            " - mAP@0.75: 0.1564\n",
            "Epoch 9, Loss: 17.0213\n",
            "Epoch 9 Metrics:\n",
            " - mAP@0.5: 0.5270\n",
            " - mAP@0.75: 0.1379\n",
            "Epoch 10, Loss: 16.6492\n",
            "Epoch 10 Metrics:\n",
            " - mAP@0.5: 0.5470\n",
            " - mAP@0.75: 0.1649\n",
            "Epoch 11, Loss: 16.1751\n",
            "Epoch 11 Metrics:\n",
            " - mAP@0.5: 0.5589\n",
            " - mAP@0.75: 0.1466\n",
            "Epoch 12, Loss: 15.9855\n",
            "Epoch 12 Metrics:\n",
            " - mAP@0.5: 0.5810\n",
            " - mAP@0.75: 0.1547\n",
            "Epoch 13, Loss: 15.3972\n",
            "Epoch 13 Metrics:\n",
            " - mAP@0.5: 0.5883\n",
            " - mAP@0.75: 0.1577\n",
            "Epoch 14, Loss: 15.0106\n",
            "Epoch 14 Metrics:\n",
            " - mAP@0.5: 0.5969\n",
            " - mAP@0.75: 0.1881\n",
            "Epoch 15, Loss: 14.7737\n",
            "Epoch 15 Metrics:\n",
            " - mAP@0.5: 0.6018\n",
            " - mAP@0.75: 0.2037\n",
            "Epoch 16, Loss: 14.4520\n",
            "Epoch 16 Metrics:\n",
            " - mAP@0.5: 0.6111\n",
            " - mAP@0.75: 0.2471\n",
            "Epoch 17, Loss: 14.1556\n",
            "Epoch 17 Metrics:\n",
            " - mAP@0.5: 0.6158\n",
            " - mAP@0.75: 0.2218\n",
            "Epoch 18, Loss: 14.0964\n",
            "Epoch 18 Metrics:\n",
            " - mAP@0.5: 0.6348\n",
            " - mAP@0.75: 0.2404\n",
            "Epoch 19, Loss: 13.8248\n",
            "Epoch 19 Metrics:\n",
            " - mAP@0.5: 0.6333\n",
            " - mAP@0.75: 0.2449\n",
            "Epoch 20, Loss: 13.7225\n",
            "Epoch 20 Metrics:\n",
            " - mAP@0.5: 0.6326\n",
            " - mAP@0.75: 0.2342\n",
            "Epoch 21, Loss: 13.4797\n",
            "Epoch 21 Metrics:\n",
            " - mAP@0.5: 0.6472\n",
            " - mAP@0.75: 0.3142\n",
            "Epoch 22, Loss: 13.3852\n",
            "Epoch 22 Metrics:\n",
            " - mAP@0.5: 0.6416\n",
            " - mAP@0.75: 0.2921\n",
            "Epoch 23, Loss: 13.3362\n",
            "Epoch 23 Metrics:\n",
            " - mAP@0.5: 0.6439\n",
            " - mAP@0.75: 0.2835\n",
            "Epoch 24, Loss: 13.1234\n",
            "Epoch 24 Metrics:\n",
            " - mAP@0.5: 0.6649\n",
            " - mAP@0.75: 0.3190\n",
            "Epoch 25, Loss: 13.1626\n",
            "Epoch 25 Metrics:\n",
            " - mAP@0.5: 0.6637\n",
            " - mAP@0.75: 0.3270\n",
            "Epoch 26, Loss: 13.0591\n",
            "Epoch 26 Metrics:\n",
            " - mAP@0.5: 0.6668\n",
            " - mAP@0.75: 0.3151\n",
            "Epoch 27, Loss: 12.9867\n",
            "Epoch 27 Metrics:\n",
            " - mAP@0.5: 0.6667\n",
            " - mAP@0.75: 0.3183\n",
            "Epoch 28, Loss: 12.9343\n",
            "Epoch 28 Metrics:\n",
            " - mAP@0.5: 0.6687\n",
            " - mAP@0.75: 0.3162\n",
            "Epoch 29, Loss: 12.8692\n",
            "Epoch 29 Metrics:\n",
            " - mAP@0.5: 0.6715\n",
            " - mAP@0.75: 0.3196\n",
            "Epoch 30, Loss: 12.7605\n",
            "Epoch 30 Metrics:\n",
            " - mAP@0.5: 0.6738\n",
            " - mAP@0.75: 0.3158\n",
            "Epoch 31, Loss: 12.8048\n",
            "Epoch 31 Metrics:\n",
            " - mAP@0.5: 0.6782\n",
            " - mAP@0.75: 0.3368\n",
            "Epoch 32, Loss: 12.7903\n",
            "Epoch 32 Metrics:\n",
            " - mAP@0.5: 0.6809\n",
            " - mAP@0.75: 0.3389\n",
            "Epoch 33, Loss: 12.7200\n",
            "Epoch 33 Metrics:\n",
            " - mAP@0.5: 0.6732\n",
            " - mAP@0.75: 0.3443\n",
            "Epoch 34, Loss: 12.6542\n",
            "Epoch 34 Metrics:\n",
            " - mAP@0.5: 0.6738\n",
            " - mAP@0.75: 0.3315\n",
            "Epoch 35, Loss: 12.6603\n",
            "Epoch 35 Metrics:\n",
            " - mAP@0.5: 0.6709\n",
            " - mAP@0.75: 0.3309\n",
            "Epoch 36, Loss: 12.6533\n",
            "Epoch 36 Metrics:\n",
            " - mAP@0.5: 0.6719\n",
            " - mAP@0.75: 0.3223\n",
            "Epoch 37, Loss: 12.6713\n",
            "Epoch 37 Metrics:\n",
            " - mAP@0.5: 0.6745\n",
            " - mAP@0.75: 0.3223\n",
            "Epoch 38, Loss: 12.7157\n",
            "Epoch 38 Metrics:\n",
            " - mAP@0.5: 0.6745\n",
            " - mAP@0.75: 0.3223\n",
            "Epoch 39, Loss: 12.6491\n",
            "Epoch 39 Metrics:\n",
            " - mAP@0.5: 0.6745\n",
            " - mAP@0.75: 0.3223\n",
            "Epoch 40, Loss: 12.6764\n",
            "Epoch 40 Metrics:\n",
            " - mAP@0.5: 0.6745\n",
            " - mAP@0.75: 0.3223\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchmetrics.detection import MeanAveragePrecision\n",
        "\n",
        "num_epochs = 40\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, weight_decay=0.05, momentum=0.9)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "# Define metric tracker\n",
        "metric = MeanAveragePrecision(iou_thresholds=[0.5, 0.75])  # COCO uses IoU 0.5:0.95\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=8,  # Smaller batch for validation\n",
        "    num_workers=2,\n",
        "    collate_fn=lambda x: tuple(zip(*x)),\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "accumulation_steps = 4\n",
        "i = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for images, targets in train_dataloader:\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        loss = sum(loss for loss in loss_dict.values())\n",
        "        loss = loss / accumulation_steps  # Scale loss\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        epoch_loss += loss.item() * accumulation_steps  # Re-scale for logging\n",
        "        i += 1\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    metric.reset()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in val_dataloader:  # Use validation set if available\n",
        "            images = [img.to(device) for img in images]\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            preds = model(images)\n",
        "            metric.update(preds, targets)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    results = metric.compute()\n",
        "    print(f\"Epoch {epoch+1} Metrics:\")\n",
        "    print(f\" - mAP@0.5: {results['map_50']:.4f}\")\n",
        "    print(f\" - mAP@0.75: {results['map_75']:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOY5mKFmNpiK",
        "outputId": "e72ba947-efdf-44f9-d34f-7c50d550d1b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f85fdf467ee0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "category_names = {1: \"Crop\", 2: \"Weed\"}\n",
        "\n",
        "def plot_predictions(image_path, predictions, threshold=0.5):\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
        "    labels = predictions[0]['labels'].cpu().numpy()\n",
        "    scores = predictions[0]['scores'].cpu().numpy()\n",
        "\n",
        "    for box, label, score in zip(boxes, labels, scores):\n",
        "        if score >= threshold:  # Filter low-confidence detections\n",
        "            x1, y1, x2, y2 = map(int, box)\n",
        "            class_name = category_names.get(label, \"Unknown\")\n",
        "            color = (0, 255, 0) if label == 1 else (255, 0, 0)  # Green for crops, red for weeds\n",
        "\n",
        "            # Draw rectangle\n",
        "            cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
        "            text = f\"{class_name}: {score:.2f}\"\n",
        "\n",
        "            # Put text above rectangle\n",
        "            cv2.putText(image, text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "def predict(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predictions = model(image_tensor)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Run inference\n",
        "image_path = \"/home/remo/Afstudeerproject/AgronomischePerformanceMeting/AnnotationAndTraining/Annotation/images/frame10.jpg\"\n",
        "predictions = predict(image_path)\n",
        "plot_predictions(image_path, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBk1hBIbNhwh"
      },
      "source": [
        "Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2k4nsRCNjJN"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"plant_dect_fastercnn_SDG_DO_0.3.pth\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}